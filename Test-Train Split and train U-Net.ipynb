{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "import rasterio\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import geopandas_osm.osm\n",
    "from descartes import PolygonPatch\n",
    "import h5py \n",
    "from scipy.misc import imresize\n",
    "import shapely.geometry\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 240 \n",
    "\n",
    "#helper functions:\n",
    "def scale_bands(img, lower_pct = 1, upper_pct = 99):\n",
    "    \"\"\"Rescale the bands of a multichannel image for display\"\"\"\n",
    "    img_scaled = np.zeros(img.shape, np.uint8)\n",
    "    for i in range(img.shape[2]):\n",
    "        band = img[:, :, i]\n",
    "        lower, upper = np.percentile(band, [lower_pct, upper_pct])\n",
    "        band = (band - lower) / (upper - lower) * 255\n",
    "        img_scaled[:, :, i] = np.clip(band, 0, 255).astype(np.uint8)\n",
    "    return img_scaled\n",
    "\n",
    "def resize(image, new_shape):\n",
    "    #img_resized = np.zeros(new_shape+(img.shape[2],)).astype('float32')\n",
    "    #for i in range(img.shape[2]):\n",
    "    #    img_resized[:, :, i] = imresize(img[:, :, i], new_shape, interp='bicubic')\n",
    "    #img_resized = cv2.resize(img,dsize=(new_shape,new_shape))\n",
    "    \n",
    "\n",
    "    r = new_shape / (1.0*image.shape[1])\n",
    "    dim = (new_shape, int(image.shape[0] * r))\n",
    "\n",
    "    # perform the actual resizing of the image and show it\n",
    "    img_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #make training masks the proper dimensionality of only 1-D\n",
    "    if len(image.shape) == 2: img_resized = np.expand_dims(img_resized,axis=2)\n",
    "    \n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#identify a training/testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make test/train split\n",
    "#### assumes image and mask files are of identical number and none are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#image_files = !ls /home/ubuntu/data/TX_post/training_tiles_v1/*img.npy\n",
    "#mask_files  = !ls /home/ubuntu/data/TX_post/training_tiles_v1/*mask.npy\n",
    "\n",
    "#do the above without jupyter magic (!)\n",
    "training_file_path = \"/home/ubuntu/data/TX_post/training_tiles/\"\n",
    "\n",
    "ls = os.listdir(training_file_path)     #list isn't in order, need to sort the output\n",
    "\n",
    "image_files = sorted([file for file in ls if file[-7:]==\"img.npy\"])  \n",
    "mask_files = sorted([file for file in ls if file[-8:]==\"mask.npy\"])\n",
    "\n",
    "def TT_file_split(image_files,mask_files,split=0.8):\n",
    "    Xf_train, Yf_train, Xf_test, Yf_test = [],[],[],[]\n",
    "    size = len(image_files)\n",
    "    indexes = range(size)\n",
    "    dice_roll = np.random.random(size)\n",
    "    for i,roll in enumerate(dice_roll):\n",
    "        if roll < split:  #add to train\n",
    "            Xf_train.append(image_files[i])\n",
    "            Yf_train.append( mask_files[i])\n",
    "        if roll >= split: #add to test\n",
    "            Xf_test.append(image_files[i])\n",
    "            Yf_test.append( mask_files[i])\n",
    "    return Xf_train, Yf_train, Xf_test, Yf_test\n",
    "\n",
    "def TT_split(image_files,mask_files,split=0.8):\n",
    "    X_train, Y_train, X_test, Y_test = [],[],[],[]\n",
    "    size = len(image_files)\n",
    "    indexes = range(size)\n",
    "    dice_roll = np.random.random(size)\n",
    "    for i,roll in enumerate(dice_roll):\n",
    "        if roll < split:  #add to train\n",
    "            X_train.append(np.load(image_files[i]))\n",
    "            Y_train.append(np.load( mask_files[i]))\n",
    "        if roll >= split: #add to test\n",
    "            X_test.append(np.load(image_files[i]))\n",
    "            Y_test.append(np.load( mask_files[i]))\n",
    "    return np.array(X_train), np.array(Y_train), np.array(X_test), np.array(Y_test)\n",
    "\n",
    "def TT_split_resize(image_files,mask_files,out_res=240,split=0.8):\n",
    "    X_train, Y_train, X_test, Y_test = [],[],[],[]\n",
    "    size = len(image_files)\n",
    "    indexes = range(size)\n",
    "    dice_roll = np.random.random(size)\n",
    "    for i,roll in enumerate(dice_roll):\n",
    "        if roll < split:  #add to train\n",
    "            X_train.append(resize(np.load(image_files[i]),out_res))\n",
    "            Y_train.append(resize(np.load(mask_files[i]),out_res))\n",
    "        if roll >= split: #add to test\n",
    "            X_test.append(resize(np.load(image_files[i]),out_res))\n",
    "            Y_test.append(resize(np.load(mask_files[i]),out_res))\n",
    "    return np.array(X_train), np.array(Y_train), np.array(X_test), np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE THE TESTING AND TRAINGING DATA SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xf_train, Yf_train, Xf_test, Yf_test  =\\\n",
    "            TT_file_split(image_files, mask_files, split=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write test and train file lists to disk\n",
    "f = open('/home/ubuntu/Notebooks/test_train_filelists/test_images.txt', 'w')\n",
    "for item in Xf_test:f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "f = open('/home/ubuntu/Notebooks/test_train_filelists/test_masks.txt', 'w')\n",
    "for item in Yf_test:f.write(\"%s\\n\" % item)\n",
    "f.close() \n",
    "f = open('/home/ubuntu/Notebooks/test_train_filelists/train_images.txt', 'w')\n",
    "for item in Xf_test:f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "f = open('/home/ubuntu/Notebooks/test_train_filelists/train_masks.txt', 'w')\n",
    "for item in Yf_test:f.write(\"%s\\n\" % item)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test  = TT_split_resize(\\\n",
    "            image_files, mask_files, out_res = 240, split=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display a random training image\n",
    "n = np.random.randint(0,len(X_train)-1) \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,14))\n",
    "ax1.imshow(X_train[n])\n",
    "ax2.imshow(Y_train[n][:,:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4528, 240, 240, 3),\n",
       " (4528, 240, 240, 1),\n",
       " (258, 240, 240, 3),\n",
       " (258, 240, 240, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782438544"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this must be a huge dataset... how big exactly?\n",
    "import sys\n",
    "sys.getsizeof(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Set network size params\n",
    "N_CLASSES = 1\n",
    "N_CHANNEL = 3\n",
    "\n",
    "# Define metrics\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Just put a negative sign in front of an accuracy metric to turn it into a loss to be minimized\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def jacc_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def jacc_coef_loss(y_true, y_pred):\n",
    "    return -jacc_coef(y_true, y_pred)\n",
    "\n",
    "def jacc_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def get_unet(lr=0.001):\n",
    "    inputs = Input((INPUT_SIZE, INPUT_SIZE, N_CHANNEL))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(N_CLASSES, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    # model.compile(optimizer=Adam(lr=lr), loss=jacc_coef_loss, metrics=[jacc_coef_int])\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=[jacc_coef_int])\n",
    "    # model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=[dice_coef])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4528, 240, 240, 3), (4528, 240, 240, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4528 samples, validate on 258 samples\n",
      "Epoch 1/1000\n",
      "4528/4528 [==============================] - 567s - loss: -876.2753 - jacc_coef_int: 55.7741 - val_loss: -726.5957 - val_jacc_coef_int: 46.5674\n",
      "Epoch 2/1000\n",
      "1175/4528 [======>.......................] - ETA: 396s - loss: -894.4341 - jacc_coef_int: 57.0948"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "# https://keras.io/callbacks/#reducelronplateau\n",
    "\n",
    "# This sets the number of training epochs (you should do a lot more than 5)\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# Define callback to save model checkpoints\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "model_checkpoint = ModelCheckpoint(os.path.join('checkpoints', 'weights.{epoch:02d}-{val_loss:.5f}.hdf5'), monitor='loss', save_best_only=True)\n",
    "\n",
    "# Define callback to reduce learning rate when learning stagnates\n",
    "# This won't actually kick in with only 5 training epochs, but I'm assuming you'll train for hundreds of epochs when you get serious about training this NN.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, epsilon=0.002, cooldown=2)\n",
    "\n",
    "# # Define rate scheduler callack (this is an alternative to ReduceLROnPlateau. There is no reason to use both.)\n",
    "# schedule = lambda epoch_i: 0.01*np.power(0.97, i)\n",
    "# schedule_lr = LearningRateScheduler(schedule)\n",
    "\n",
    "# TensorBoard visuluaziations... this stuff is so freaking cool\n",
    "tensorboard = TensorBoard(log_dir='/tmp/tboard_logs2', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# Train the model\n",
    "model = get_unet(0.001)\n",
    "model.fit(X_train, Y_train, batch_size=25, epochs=NUM_EPOCHS, verbose=1, shuffle=True, callbacks=[model_checkpoint, reduce_lr, tensorboard], validation_data=(X_test, Y_test))\n",
    "\n",
    "print (\"total runtime = \",datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
